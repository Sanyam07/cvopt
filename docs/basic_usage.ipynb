{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================\n",
    "   \n",
    "__Contents__\n",
    "* Search usage\n",
    "   0. Import module & Load data\n",
    "   1. Defining parameter search space\n",
    "   2. Defining feature search space (optional)\n",
    "   3. Run search\n",
    "   \n",
    "* Log usage\n",
    "   1. Extract pramater & feature setting\n",
    "   2. Make meta feature for stacking\n",
    "   \n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import module & Load data\n",
    "In here, use the breast cancer wisconsin dataset to modeling.   \n",
    "This is binary classification dataset.   \n",
    "Firstly, this dataset is splitted to three datasets(Train, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np, pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook() # When you need search visualization, need run output_notebook()\n",
    "\n",
    "from hyperopt import hp, tpe, rand\n",
    "from cvopt.model_selection import hyperoptCV\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(dataset.data, dataset.target, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Test features shape:\", Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining parameter search space\n",
    "In cvopt, a Paraneter search space definition depend on base module of cross validation class.   \n",
    "When use hyperoptCV, you define [hyperopt format search space](https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"penalty\": hp.choice(\"penalty\", ['l1', 'l2']),\n",
    "    \"C\": hp.loguniform(\"C\", -3, 3), \n",
    "    \"tol\" : hp.loguniform(\"tol\", -4, -2), \n",
    "    \"class_weight\" : hp.choice(\"class_weight\", [None, \"balanced\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining feature search space (optional)\n",
    "Features are selected per feature group.   \n",
    "__If feature group is set \"-100\", this group's features always are used.__   \n",
    "Criterion of separating group is, for example, random, difference of feature engineering method or difference of data source.\n",
    "   \n",
    "When you don't set featured group, optimizer use all input features.\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "### Example.   \n",
    "When data has 5 features(5 cols) and feature group is set as shown below.\n",
    "   \n",
    "| feature index(data col index) | feature group |   \n",
    "|:------------:|:------------:|   \n",
    "| 0 | 0 |\n",
    "| 1 | 0 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n",
    "   \n",
    "Define as follows python's list.   \n",
    "   \n",
    "```feature_groups = [0, 0, 0, 1, 1]```\n",
    "\n",
    "As search result, you may get flag per feature group.\n",
    "   \n",
    "```feature_groups0: True   \n",
    "feature_groups1: False```\n",
    "   \n",
    "This result means that optimizer recommend using group 1 features(col index:0,1,2) and not using group 2.\n",
    "\n",
    "------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = np.random.randint(0, 5, Xtrain.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run search\n",
    "cvopt has API like scikit-learn cross validation class.   \n",
    "When you had use scikit-learn, you can use cvopt very easy.\n",
    "   \n",
    "For each optimizer class's detail, please see [API reference]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "hpcv = hyperoptCV(estimator, param_distributions, \n",
    "                  scoring=\"roc_auc\",               # Objective of search\n",
    "                  cv=cv,                           # Cross validation setting\n",
    "                  max_evals=32,                    # Number of search\n",
    "                  n_jobs=3,                        # Number of jobs to run in parallel.\n",
    "                  random_state=0,                  # seed in hyperopt\n",
    "                  verbose=2,                       # 0: don't display status, 1:display status by stdout, 2:display status by graph \n",
    "                  logdir=\"./search_usage\",         # If this path is specified, save the log.\n",
    "                  model_id=\"search_usage\",         # used estimator's dir and file name in save.\n",
    "                  save_estimator=2,\n",
    "                  )\n",
    "\n",
    "hpcv.fit(Xtrain, ytrain, validation_data=(Xtest, ytest), \n",
    "         # validation_data is optional.\n",
    "         # This data is only used to compute validation score(don't fit).\n",
    "         # When this data is input & save_estimator=True,the estimator which is fitted whole Xtrain is saved.\n",
    "         feature_groups=feature_groups)\n",
    "ytest_pred = hpcv.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hpcv.cv_results_).head() # Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract pramater & feature setting\n",
    "In cvopt, helper function is Included to handle log file easily.   \n",
    "When you want to extract settings from log file, It can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvopt.utils import extract_params\n",
    "estimator_params, feature_params, feature_select_flag  = extract_params(logdir=\"./search_usage\", \n",
    "                                                                        model_id=\"search_usage\", \n",
    "                                                                        target_index=0, \n",
    "                                                                        feature_groups=feature_groups)\n",
    "\n",
    "estimator.set_params(**estimator_params)         # Set estimator parameters\n",
    "Xtrain_selected = Xtrain[:, feature_select_flag] # Extract selected feature columns\n",
    "\n",
    "print(estimator)\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Train selected features shape:\",Xtrain_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make meta feature for stacking\n",
    "When you want to male mete feature and [stacking](https://mlwave.com/kaggle-ensembling-guide/), It can be implemented as follows.   \n",
    "   \n",
    "When run search, You need set \"save_estimator>0\" to make meta feature.    \n",
    "In addition, you need set \"save_estimator>1\" to make meta feature from the data which is not fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvopt.utils import mk_metafeature\n",
    "Xtrain_meta, Xtest_meta = mk_metafeature(Xtrain, ytrain, \n",
    "                                         logdir=\"./search_usage\", \n",
    "                                         model_id=\"search_usage\", \n",
    "                                         target_index=0, \n",
    "                                         cv=cv, \n",
    "                                         validation_data=(Xtest, ytest), \n",
    "                                         feature_groups=feature_groups, \n",
    "                                         estimator_method=\"predict_proba\")\n",
    "\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Train meta features shape:\", Xtrain_meta.shape)\n",
    "print(\"Test features shape:\", Xtest.shape)\n",
    "print(\"Test meta features shape:\",  Xtest_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

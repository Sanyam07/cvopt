{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================\n",
    "   \n",
    "__Contents__\n",
    "* Search usage\n",
    "   1. Import module & Load data\n",
    "   2. Defining parameter search space\n",
    "   3. Defining feature search space (optional)\n",
    "   4. Run search\n",
    "   \n",
    "* Log usage\n",
    "   1. Extract pramater & feature setting\n",
    "   2. Make meta feature for stacking\n",
    "   \n",
    "* Sample:run all backend search\n",
    "\n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import module & Load data\n",
    "In here, use the breast cancer wisconsin dataset to modeling.   \n",
    "This is binary classification dataset.   \n",
    "Firstly, this dataset is splitted to three datasets(Train, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,sys\n",
    "import numpy as np, pandas as pd, scipy as sp\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from cvopt.model_selection import SimpleoptCV\n",
    "from cvopt.search_setting import search_category, search_numeric\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(dataset.data, dataset.target, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Test features shape:\", Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook() # When you need search visualization, need run output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining parameter search space\n",
    "Can use a common style in all cv class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"penalty\": search_category(['none', 'l2']),\n",
    "    \"C\": search_numeric(0.01, 3.0, \"float\"), \n",
    "    \"tol\" : search_numeric(0.0001, 0.001, \"float\"),  \n",
    "    \"class_weight\" : search_category([None, \"balanced\", {0:0.5, 1:0.1}]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A Other styles\n",
    "Can use other styles depend on base module. \n",
    "\n",
    "### for HyperoptCV (base module: Hyperopt)\n",
    "```python\n",
    "param_distributions = {\n",
    "    \"penalty\": hp.choice(\"penalty\", ['none', 'l2']),\n",
    "    \"C\": hp.loguniform(\"C\", 0.01, 3.0), \n",
    "    \"tol\" : hp.loguniform(\"tol\", 0.0001, 0.001), \n",
    "    \"class_weight\" : hp.choice(\"class_weight\", [None, \"balanced\", {0:0.5, 1:0.1}]),\n",
    "    }\n",
    "```\n",
    "### for  BayesoptCV (base module: GpyOpt)\n",
    "__NOTE:__\n",
    "* In GpyOpt, search space is list of dict. But in cvopt, need dict of dict(key:param name, value:dict).\n",
    "* If `type` is `categorical`,  search space's dict must have `categories` key. `categories` value is category name's list.\n",
    "```python\n",
    "param_distributions = {\n",
    "    \"penalty\" : {\"name\": \"penalty\", \"type\":\"categorical\", \"domain\":(0,1), \"categories\":['none', 'l2']},\n",
    "    \"C\": {\"name\": \"C\", \"type\":\"continuous\", \"domain\":(0.01, 3.0)}, \n",
    "    \"tol\" : {\"name\": \"tol\", \"type\":\"continuous\", \"domain\":(0.0001, 0.001)}, \n",
    "    \"class_weight\" : {\"name\": \"class_weight\", \"type\":\"categorical\", \"domain\":(0,1), \"categories\":[None, \"balanced\", {0:0.5, 1:0.1}]},\n",
    "    }\n",
    "```\n",
    "\n",
    "### for  GAoptCV, RandomoptCV\n",
    "__NOTE:__\n",
    "* Support search_setting.search_numeric, search_setting.search_category, or scipy.stats class.\n",
    "```python\n",
    "param_distributions = {\n",
    "    \"penalty\" : hp.choice(\"penalty\", ['none', 'l2']),\n",
    "    \"C\": sp.stats.randint(low=0.01, high=3.0), \n",
    "    \"tol\" : sp.stats.uniform(loc=0.0001, scale=0.00009),  \n",
    "    \"class_weight\" :  hp.choice(\"class_weight\", [None, \"balanced\", {0:0.5, 1:0.1}]),\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining feature search space (optional)\n",
    "Features are selected per `feature_group`.   \n",
    "__If `feature_group` is set \"-1\", this group's features always are used.__   \n",
    "Criterion of separating group is, for example, random, difference of feature engineering method or difference of data source.\n",
    "   \n",
    "When you don't set `feature_group`, optimizer use all input features.\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "### Example.   \n",
    "When data has 5 features(5 cols) and `feature_group` is set as shown below.\n",
    "   \n",
    "| feature index(data col index) | feature group |   \n",
    "|:------------:|:------------:|   \n",
    "| 0 | 0 |\n",
    "| 1 | 0 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n",
    "   \n",
    "Define as follows python's list.   \n",
    "   \n",
    "```\n",
    "feature_groups = [0, 0, 0, 1, 1]\n",
    "```\n",
    "\n",
    "As search result, you may get flag per `feature_group`.\n",
    "   \n",
    "```\n",
    "feature_groups0: True   \n",
    "feature_groups1: False\n",
    "```\n",
    "   \n",
    "This result means that optimizer recommend using group 1 features(col index:0,1,2) and not using group 2.\n",
    "\n",
    "------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = np.random.randint(0, 5, Xtrain.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run search\n",
    "cvopt has API like scikit-learn cross validation class.   \n",
    "When you had use scikit-learn, you can use cvopt very easy.\n",
    "   \n",
    "For each optimizer class's detail, please see [API reference](https://genfifth.github.io/cvopt/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "opt = SimpleoptCV(estimator, param_distributions, \n",
    "                 scoring=\"roc_auc\",              # Objective of search\n",
    "                 cv=cv,                          # Cross validation setting\n",
    "                 max_iter=32,                    # Number of search\n",
    "                 n_jobs=3,                       # Number of jobs to run in parallel.\n",
    "                 verbose=2,                      # 0: don't display status, 1:display status by stdout, 2:display status by graph \n",
    "                 logdir=\"./search_usage\",        # If this path is specified, save the log.\n",
    "                 model_id=\"search_usage\",        # used estimator's dir and file name in save.\n",
    "                 save_estimator=2,               # estimator save setting.\n",
    "                 backend=\"hyperopt\",             # hyperopt,bayesopt, gaopt or randomopt.\n",
    "                 )\n",
    "\n",
    "opt.fit(Xtrain, ytrain, validation_data=(Xtest, ytest), \n",
    "        # validation_data is optional.\n",
    "        # This data is only used to compute validation score(don't fit).\n",
    "        # When this data is input & save_estimator=True,the estimator which is fitted whole Xtrain is saved.\n",
    "        feature_groups=feature_groups, \n",
    "        )\n",
    "ytest_pred = opt.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(opt.cv_results_).head() # Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract pramater & feature setting\n",
    "In cvopt, helper function is Included to handle log file easily.   \n",
    "When you want to extract settings from log file, It can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvopt.utils import extract_params\n",
    "target_index = pd.DataFrame(opt.cv_results_)[pd.DataFrame(opt.cv_results_)[\"mean_test_score\"] == opt.best_score_][\"index\"].values[0]\n",
    "\n",
    "estimator_params, feature_params, feature_select_flag  = extract_params(logdir=\"./search_usage\", \n",
    "                                                                        model_id=\"search_usage\", \n",
    "                                                                        target_index=target_index, \n",
    "                                                                        feature_groups=feature_groups)\n",
    "\n",
    "estimator.set_params(**estimator_params)         # Set estimator parameters\n",
    "Xtrain_selected = Xtrain[:, feature_select_flag] # Extract selected feature columns\n",
    "\n",
    "print(estimator)\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Train selected features shape:\",Xtrain_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make meta feature for stacking\n",
    "When you want to male mete feature and [stacking](https://mlwave.com/kaggle-ensembling-guide/), It can be implemented as follows.   \n",
    "   \n",
    "When run search, You need set `save_estimator`>0 to make meta feature.    \n",
    "In addition, you need set `save_estimator`>1 to make meta feature from the data which is not fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvopt.utils import mk_metafeature\n",
    "target_index = pd.DataFrame(opt.cv_results_)[pd.DataFrame(opt.cv_results_)[\"mean_test_score\"] == opt.best_score_][\"index\"].values[0]\n",
    "\n",
    "Xtrain_meta, Xtest_meta = mk_metafeature(Xtrain, ytrain, \n",
    "                                         logdir=\"./search_usage\", \n",
    "                                         model_id=\"search_usage\", \n",
    "                                         target_index=target_index, \n",
    "                                         cv=cv, \n",
    "                                         validation_data=(Xtest, ytest), \n",
    "                                         feature_groups=feature_groups, \n",
    "                                         estimator_method=\"predict_proba\")\n",
    "\n",
    "print(\"Train features shape:\", Xtrain.shape)\n",
    "print(\"Train meta features shape:\", Xtrain_meta.shape)\n",
    "print(\"Test features shape:\", Xtest.shape)\n",
    "print(\"Test meta features shape:\",  Xtest_meta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample:run all backend search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bk in [\"hyperopt\", \"gaopt\", \"bayesopt\", \"randomopt\"]:\n",
    "    estimator = LogisticRegression()\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "    opt = SimpleoptCV(estimator, param_distributions, \n",
    "                     scoring=\"roc_auc\",              # Objective of search\n",
    "                     cv=cv,                          # Cross validation setting\n",
    "                     max_iter=32,                    # Number of search\n",
    "                     n_jobs=3,                       # Number of jobs to run in parallel.\n",
    "                     verbose=2,                      # 0: don't display status, 1:display status by stdout, 2:display status by graph \n",
    "                     logdir=\"./search_usage\",        # If this path is specified, save the log.\n",
    "                     model_id=bk,                    # used estimator's dir and file name in save.\n",
    "                     save_estimator=2,               # estimator save setting.\n",
    "                     backend=bk,                     # hyperopt,bayesopt, gaopt or randomopt.\n",
    "                     )\n",
    "\n",
    "    opt.fit(Xtrain, ytrain, validation_data=(Xtest, ytest), \n",
    "            # validation_data is optional.\n",
    "            # This data is only used to compute validation score(don't fit).\n",
    "            # When this data is input & save_estimator=True,the estimator which is fitted whole Xtrain is saved.\n",
    "            feature_groups=feature_groups, \n",
    "            )\n",
    "    ytest_pred = opt.predict(Xtest)\n",
    "\n",
    "\n",
    "    from cvopt.utils import extract_params\n",
    "    target_index = pd.DataFrame(opt.cv_results_)[pd.DataFrame(opt.cv_results_)[\"mean_test_score\"] == opt.best_score_][\"index\"].values[0]\n",
    "\n",
    "    estimator_params, feature_params, feature_select_flag  = extract_params(logdir=\"./search_usage\", \n",
    "                                                                            model_id=bk, \n",
    "                                                                            target_index=target_index, \n",
    "                                                                            feature_groups=feature_groups)\n",
    "\n",
    "    estimator.set_params(**estimator_params)         # Set estimator parameters\n",
    "    Xtrain_selected = Xtrain[:, feature_select_flag] # Extract selected feature columns\n",
    "\n",
    "    print(estimator)\n",
    "    print(\"Train features shape:\", Xtrain.shape)\n",
    "    print(\"Train selected features shape:\",Xtrain_selected.shape)\n",
    "\n",
    "    from cvopt.utils import mk_metafeature\n",
    "    Xtrain_meta, Xtest_meta = mk_metafeature(Xtrain, ytrain, \n",
    "                                             logdir=\"./search_usage\", \n",
    "                                             model_id=bk, \n",
    "                                             target_index=target_index, \n",
    "                                             cv=cv, \n",
    "                                             validation_data=(Xtest, ytest), \n",
    "                                             feature_groups=feature_groups, \n",
    "                                             estimator_method=\"predict_proba\")\n",
    "\n",
    "    print(\"Train features shape:\", Xtrain.shape)\n",
    "    print(\"Train meta features shape:\", Xtrain_meta.shape)\n",
    "    print(\"Test features shape:\", Xtest.shape)\n",
    "    print(\"Test meta features shape:\",  Xtest_meta.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
